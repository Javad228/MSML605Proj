{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcCu_nafX-jN",
        "outputId": "1ee1f653-c001-490b-c824-55c98e4543e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-1253a1ea-fc4c-cb78-baa0-e96f0f966cc0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install -qq -y nvidia-cuda-toolkit build-essential\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWWkGXP0YAhT",
        "outputId": "e4ac0354-0d2c-44c0-b255-461610c42721"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Extracting templates from packages: 100%\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package libdebuginfod-common.\n",
            "(Reading database ... 126102 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libdebuginfod-common_0.186-1ubuntu0.1_all.deb ...\n",
            "Unpacking libdebuginfod-common (0.186-1ubuntu0.1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../01-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../02-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libcupti11.5:amd64.\n",
            "Preparing to unpack .../03-libcupti11.5_11.5.114~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcupti11.5:amd64 (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libaccinj64-11.5:amd64.\n",
            "Preparing to unpack .../04-libaccinj64-11.5_11.5.114~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libaccinj64-11.5:amd64 (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../05-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../06-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../07-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../08-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../09-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libcub-dev.\n",
            "Preparing to unpack .../10-libcub-dev_1.15.0-3_all.deb ...\n",
            "Unpacking libcub-dev (1.15.0-3) ...\n",
            "Selecting previously unselected package libcublaslt11:amd64.\n",
            "Preparing to unpack .../11-libcublaslt11_11.7.4.6~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcublaslt11:amd64 (11.7.4.6~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libcublas11:amd64.\n",
            "Preparing to unpack .../12-libcublas11_11.7.4.6~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcublas11:amd64 (11.7.4.6~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libcudart11.0:amd64.\n",
            "Preparing to unpack .../13-libcudart11.0_11.5.117~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcudart11.0:amd64 (11.5.117~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libcufft10:amd64.\n",
            "Preparing to unpack .../14-libcufft10_11.1.1+~10.6.0.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcufft10:amd64 (11.1.1+~10.6.0.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libcufftw10:amd64.\n",
            "Preparing to unpack .../15-libcufftw10_11.1.1+~10.6.0.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcufftw10:amd64 (11.1.1+~10.6.0.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-compute-535:amd64.\n",
            "Preparing to unpack .../16-libnvidia-compute-535_535.247.01-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-compute-535:amd64 (535.247.01-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-compute-510:amd64.\n",
            "Preparing to unpack .../17-libnvidia-compute-510_525.147.05-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking libnvidia-compute-510:amd64 (525.147.05-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package libnvidia-compute-495:amd64.\n",
            "Preparing to unpack .../18-libnvidia-compute-495_510.108.03-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libnvidia-compute-495:amd64 (510.108.03-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libcuinj64-11.5:amd64.\n",
            "Preparing to unpack .../19-libcuinj64-11.5_11.5.114~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcuinj64-11.5:amd64 (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libcurand10:amd64.\n",
            "Preparing to unpack .../20-libcurand10_11.1.1+~10.2.7.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcurand10:amd64 (11.1.1+~10.2.7.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libcusolver11:amd64.\n",
            "Preparing to unpack .../21-libcusolver11_11.3.2.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcusolver11:amd64 (11.3.2.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libcusolvermg11:amd64.\n",
            "Preparing to unpack .../22-libcusolvermg11_11.3.2.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcusolvermg11:amd64 (11.3.2.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libcusparse11:amd64.\n",
            "Preparing to unpack .../23-libcusparse11_11.7.0.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcusparse11:amd64 (11.7.0.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libdebuginfod1:amd64.\n",
            "Preparing to unpack .../24-libdebuginfod1_0.186-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libdebuginfod1:amd64 (0.186-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libglx-dev:amd64.\n",
            "Preparing to unpack .../25-libglx-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglx-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl-dev:amd64.\n",
            "Preparing to unpack .../26-libgl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libegl-dev:amd64.\n",
            "Preparing to unpack .../27-libegl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libegl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "Preparing to unpack .../28-libgtk2.0-common_2.24.33-2ubuntu2.1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../29-libgtk2.0-0_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../30-libgail18_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../31-libgail-common_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgles1:amd64.\n",
            "Preparing to unpack .../32-libgles1_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles1:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles-dev:amd64.\n",
            "Preparing to unpack .../33-libgles-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../34-libgtk2.0-bin_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libipt2.\n",
            "Preparing to unpack .../35-libipt2_2.0.5-1_amd64.deb ...\n",
            "Unpacking libipt2 (2.0.5-1) ...\n",
            "Selecting previously unselected package libnppc11:amd64.\n",
            "Preparing to unpack .../36-libnppc11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnppc11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnppial11:amd64.\n",
            "Preparing to unpack .../37-libnppial11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnppial11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnppicc11:amd64.\n",
            "Preparing to unpack .../38-libnppicc11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnppicc11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnppidei11:amd64.\n",
            "Preparing to unpack .../39-libnppidei11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnppidei11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnppif11:amd64.\n",
            "Preparing to unpack .../40-libnppif11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnppif11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnppig11:amd64.\n",
            "Preparing to unpack .../41-libnppig11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnppig11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnppim11:amd64.\n",
            "Preparing to unpack .../42-libnppim11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnppim11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnppist11:amd64.\n",
            "Preparing to unpack .../43-libnppist11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnppist11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnppisu11:amd64.\n",
            "Preparing to unpack .../44-libnppisu11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnppisu11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnppitc11:amd64.\n",
            "Preparing to unpack .../45-libnppitc11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnppitc11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnpps11:amd64.\n",
            "Preparing to unpack .../46-libnpps11_11.5.1.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnpps11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnvblas11:amd64.\n",
            "Preparing to unpack .../47-libnvblas11_11.7.4.6~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnvblas11:amd64 (11.7.4.6~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-ml-dev:amd64.\n",
            "Preparing to unpack .../48-libnvidia-ml-dev_11.5.50~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnvidia-ml-dev:amd64 (11.5.50~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnvjpeg11:amd64.\n",
            "Preparing to unpack .../49-libnvjpeg11_11.5.4.107~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnvjpeg11:amd64 (11.5.4.107~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnvrtc-builtins11.5:amd64.\n",
            "Preparing to unpack .../50-libnvrtc-builtins11.5_11.5.119~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnvrtc-builtins11.5:amd64 (11.5.119~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnvrtc11.2:amd64.\n",
            "Preparing to unpack .../51-libnvrtc11.2_11.5.119~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnvrtc11.2:amd64 (11.5.119~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libnvvm4:amd64.\n",
            "Preparing to unpack .../52-libnvvm4_11.5.119~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnvvm4:amd64 (11.5.119~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libopengl-dev:amd64.\n",
            "Preparing to unpack .../53-libopengl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package librsvg2-common:amd64.\n",
            "Preparing to unpack .../54-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
            "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Selecting previously unselected package libsource-highlight-common.\n",
            "Preparing to unpack .../55-libsource-highlight-common_3.1.9-4.1build2_all.deb ...\n",
            "Unpacking libsource-highlight-common (3.1.9-4.1build2) ...\n",
            "Selecting previously unselected package libsource-highlight4v5.\n",
            "Preparing to unpack .../56-libsource-highlight4v5_3.1.9-4.1build2_amd64.deb ...\n",
            "Unpacking libsource-highlight4v5 (3.1.9-4.1build2) ...\n",
            "Selecting previously unselected package libvdpau-dev:amd64.\n",
            "Preparing to unpack .../57-libvdpau-dev_1.4-3build2_amd64.deb ...\n",
            "Unpacking libvdpau-dev:amd64 (1.4-3build2) ...\n",
            "Selecting previously unselected package node-html5shiv.\n",
            "Preparing to unpack .../58-node-html5shiv_3.7.3+dfsg-4_all.deb ...\n",
            "Unpacking node-html5shiv (3.7.3+dfsg-4) ...\n",
            "Selecting previously unselected package nvidia-cuda-toolkit-doc.\n",
            "Preparing to unpack .../59-nvidia-cuda-toolkit-doc_11.5.1-1ubuntu1_all.deb ...\n",
            "Unpacking nvidia-cuda-toolkit-doc (11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../60-openjdk-8-jre-headless_8u452-ga~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u452-ga~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jre:amd64.\n",
            "Preparing to unpack .../61-openjdk-8-jre_8u452-ga~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre:amd64 (8u452-ga~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libbabeltrace1:amd64.\n",
            "Preparing to unpack .../62-libbabeltrace1_1.5.8-2build1_amd64.deb ...\n",
            "Unpacking libbabeltrace1:amd64 (1.5.8-2build1) ...\n",
            "Selecting previously unselected package libcupti-dev:amd64.\n",
            "Preparing to unpack .../63-libcupti-dev_11.5.114~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcupti-dev:amd64 (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libcupti-doc.\n",
            "Preparing to unpack .../64-libcupti-doc_11.5.114~11.5.1-1ubuntu1_all.deb ...\n",
            "Unpacking libcupti-doc (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libglvnd-core-dev:amd64.\n",
            "Preparing to unpack .../65-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-dev:amd64.\n",
            "Preparing to unpack .../66-libglvnd-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl1-mesa-dev:amd64.\n",
            "Preparing to unpack .../67-libgl1-mesa-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Selecting previously unselected package libnvtoolsext1:amd64.\n",
            "Preparing to unpack .../68-libnvtoolsext1_11.5.114~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libnvtoolsext1:amd64 (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libthrust-dev.\n",
            "Preparing to unpack .../69-libthrust-dev_1.15.0-1_all.deb ...\n",
            "Unpacking libthrust-dev (1.15.0-1) ...\n",
            "Selecting previously unselected package nvidia-cuda-dev:amd64.\n",
            "Preparing to unpack .../70-nvidia-cuda-dev_11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-cuda-dev:amd64 (11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-cuda-gdb.\n",
            "Preparing to unpack .../71-nvidia-cuda-gdb_11.5.114~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-cuda-gdb (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-profiler.\n",
            "Preparing to unpack .../72-nvidia-profiler_11.5.114~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-profiler (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-cuda-toolkit.\n",
            "Preparing to unpack .../73-nvidia-cuda-toolkit_11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-cuda-toolkit (11.5.1-1ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-visual-profiler.\n",
            "Preparing to unpack .../74-nvidia-visual-profiler_11.5.114~11.5.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-visual-profiler (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Setting up libcusparse11:amd64 (11.7.0.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libdebuginfod-common (0.186-1ubuntu0.1) ...\n",
            "\n",
            "Creating config file /etc/profile.d/debuginfod.sh with new version\n",
            "\n",
            "Creating config file /etc/profile.d/debuginfod.csh with new version\n",
            "Setting up libnppc11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libdebuginfod1:amd64 (0.186-1ubuntu0.1) ...\n",
            "Setting up node-html5shiv (3.7.3+dfsg-4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libcupti-doc (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Setting up libsource-highlight-common (3.1.9-4.1build2) ...\n",
            "Setting up libcudart11.0:amd64 (11.5.117~11.5.1-1ubuntu1) ...\n",
            "Setting up libnppisu11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libnppicc11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libnvjpeg11:amd64 (11.5.4.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libcublaslt11:amd64 (11.7.4.6~11.5.1-1ubuntu1) ...\n",
            "Setting up libgles1:amd64 (1.4.0-1) ...\n",
            "Setting up libcupti11.5:amd64 (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u452-ga~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up libipt2 (2.0.5-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up libbabeltrace1:amd64 (1.5.8-2build1) ...\n",
            "Setting up libnpps11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libnppim11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libcufft10:amd64 (11.1.1+~10.6.0.107~11.5.1-1ubuntu1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up libnppitc11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libnppist11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libglx-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libnvidia-compute-535:amd64 (535.247.01-0ubuntu1) ...\n",
            "Setting up libnvvm4:amd64 (11.5.119~11.5.1-1ubuntu1) ...\n",
            "Setting up libvdpau-dev:amd64 (1.4-3build2) ...\n",
            "Setting up libnvtoolsext1:amd64 (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libcub-dev (1.15.0-3) ...\n",
            "Setting up libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up nvidia-cuda-toolkit-doc (11.5.1-1ubuntu1) ...\n",
            "Setting up libaccinj64-11.5:amd64 (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Setting up libnppig11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libgl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libcurand10:amd64 (11.1.1+~10.2.7.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libsource-highlight4v5 (3.1.9-4.1build2) ...\n",
            "Setting up libnvrtc-builtins11.5:amd64 (11.5.119~11.5.1-1ubuntu1) ...\n",
            "Setting up libnppidei11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libthrust-dev (1.15.0-1) ...\n",
            "Setting up libnppial11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libnppif11:amd64 (11.5.1.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libcufftw10:amd64 (11.1.1+~10.6.0.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libcublas11:amd64 (11.7.4.6~11.5.1-1ubuntu1) ...\n",
            "Setting up nvidia-cuda-gdb (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Setting up libegl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libnvidia-compute-510:amd64 (525.147.05-0ubuntu2.22.04.1) ...\n",
            "Setting up libcupti-dev:amd64 (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Setting up libnvidia-compute-495:amd64 (510.108.03-0ubuntu0.22.04.1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up libnvblas11:amd64 (11.7.4.6~11.5.1-1ubuntu1) ...\n",
            "Setting up libcusolver11:amd64 (11.3.2.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libnvrtc11.2:amd64 (11.5.119~11.5.1-1ubuntu1) ...\n",
            "Setting up libcusolvermg11:amd64 (11.3.2.107~11.5.1-1ubuntu1) ...\n",
            "Setting up libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgles-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libcuinj64-11.5:amd64 (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Setting up libnvidia-ml-dev:amd64 (11.5.50~11.5.1-1ubuntu1) ...\n",
            "Setting up libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up nvidia-cuda-dev:amd64 (11.5.1-1ubuntu1) ...\n",
            "Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Setting up openjdk-8-jre:amd64 (8u452-ga~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/policytool to provide /usr/bin/policytool (policytool) in auto mode\n",
            "Setting up nvidia-profiler (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Setting up nvidia-cuda-toolkit (11.5.1-1ubuntu1) ...\n",
            "Setting up libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up nvidia-visual-profiler (11.5.114~11.5.1-1ubuntu1) ...\n",
            "Processing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.3) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download from Google Cloud mirror\n",
        "!wget -q https://storage.googleapis.com/cvdf-datasets/mnist/t10k-images-idx3-ubyte.gz \\\n",
        "             https://storage.googleapis.com/cvdf-datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
        "\n",
        "# Check they’re here\n",
        "!ls -1 t10k-*\n",
        "\n",
        "# Unzip\n",
        "!gunzip -f t10k-images-idx3-ubyte.gz t10k-labels-idx1-ubyte.gz\n",
        "\n",
        "# Verify\n",
        "!ls -1 t10k-*\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fLnrO4gYl0G",
        "outputId": "fe9a765e-8f70-40e1-ad63-4ff695c435ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t10k-images-idx3-ubyte.gz\n",
            "t10k-labels-idx1-ubyte.gz\n",
            "t10k-images-idx3-ubyte\n",
            "t10k-labels-idx1-ubyte\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training scripts to generate the model params"
      ],
      "metadata": {
        "id": "n2eLZRskgE4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting NHWC format params"
      ],
      "metadata": {
        "id": "3PNVSxLVglNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "BATCH  = 128\n",
        "EPOCHS = 5\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class TinyCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1,  8, 3, padding=1)   # 1→8\n",
        "        self.fc    = nn.Linear(14*14*8, 10)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = torch.relu(self.conv1(x))          # 28×28\n",
        "        x = torch.max_pool2d(x, 2)             # 14×14\n",
        "        x = self.fc(x.view(x.size(0), -1))\n",
        "        return x\n",
        "\n",
        "net = TinyCNN().to(DEVICE)\n",
        "\n",
        "tr = transforms.Compose([transforms.ToTensor()])\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\".\", train=True,  download=True, transform=tr),\n",
        "    batch_size=BATCH, shuffle=True)\n",
        "test_loader  = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\".\", train=False, download=True, transform=tr),\n",
        "    batch_size=1000, shuffle=False)\n",
        "\n",
        "opt = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    net.train()\n",
        "    for x,y in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\"):\n",
        "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
        "        opt.zero_grad(); loss_fn(net(x), y).backward(); opt.step()\n",
        "\n",
        "net.eval(); correct = total = 0\n",
        "with torch.no_grad():\n",
        "    for x,y in test_loader:\n",
        "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
        "        preds = net(x).argmax(1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total   += y.size(0)\n",
        "print(f\"Test accuracy: {100*correct/total:.2f}%\")\n",
        "\n",
        "def save_raw(fname, tensor):\n",
        "    tensor.cpu().contiguous().detach().numpy().astype(\"f\").tofile(fname)\n",
        "\n",
        "save_raw(\"conv1.w\", net.conv1.weight.permute(0,2,3,1))\n",
        "save_raw(\"conv1.b\", net.conv1.bias)\n",
        "\n",
        "save_raw(\"fc.w\",  net.fc.weight)\n",
        "\n",
        "perm = []\n",
        "for h in range(14):\n",
        "    for w in range(14):\n",
        "        for c in range(8):\n",
        "            perm.append(c*14*14 + h*14 + w)\n",
        "\n",
        "# after permuting / re-ordering:\n",
        "fc_w_reordered = net.fc.weight[:, perm]\n",
        "\n",
        "# move to CPU, detach, convert to numpy, cast to float32, then dump\n",
        "fc_w_reordered_np = fc_w_reordered.cpu().detach().numpy().astype('f')\n",
        "fc_w_reordered_np.tofile(\"fc.w\")\n",
        "\n",
        "print(\"fc.w rewritten in H‑W‑C order\")\n",
        "\n",
        "save_raw(\"fc.b\",  net.fc.bias)\n",
        "\n",
        "print(\"Weights written: conv1.w/.b  fc.w/.b\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vFF93KUf2qS",
        "outputId": "08329f7e-7400-42c8-d6ef-afba05ae9d96"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.51MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 64.6kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:06<00:00, 242kB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.37MB/s]\n",
            "Epoch 1/5: 100%|██████████| 469/469 [00:07<00:00, 62.99it/s]\n",
            "Epoch 2/5: 100%|██████████| 469/469 [00:07<00:00, 65.16it/s]\n",
            "Epoch 3/5: 100%|██████████| 469/469 [00:06<00:00, 71.37it/s]\n",
            "Epoch 4/5: 100%|██████████| 469/469 [00:07<00:00, 65.93it/s]\n",
            "Epoch 5/5: 100%|██████████| 469/469 [00:07<00:00, 64.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 96.92%\n",
            "fc.w rewritten in H‑W‑C order\n",
            "Weights written: conv1.w/.b  fc.w/.b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting NCHW format params"
      ],
      "metadata": {
        "id": "CaEYtB8ngphG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "BATCH  = 128\n",
        "EPOCHS = 5\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class TinyCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1,  8, 3, padding=1)\n",
        "        self.fc    = nn.Linear(14*14*8, 10)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = self.fc(x.view(x.size(0), -1))\n",
        "        return x\n",
        "\n",
        "net = TinyCNN().to(DEVICE)\n",
        "tr = transforms.Compose([transforms.ToTensor()])\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\".\", train=True,  download=True, transform=tr),\n",
        "    batch_size=BATCH, shuffle=True)\n",
        "test_loader  = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\".\", train=False, download=True, transform=tr),\n",
        "    batch_size=1000, shuffle=False)\n",
        "\n",
        "opt = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    net.train()\n",
        "    for x,y in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\"):\n",
        "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
        "        opt.zero_grad(); loss_fn(net(x), y).backward(); opt.step()\n",
        "print(\"Training done.\")\n",
        "\n",
        "net.eval(); correct = total = 0\n",
        "with torch.no_grad():\n",
        "    for x,y in test_loader:\n",
        "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
        "        preds = net(x).argmax(1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total   += y.size(0)\n",
        "print(f\"Test accuracy: {100*correct/total:.2f}%\")\n",
        "\n",
        "def save_raw(fname, tensor):\n",
        "    tensor.cpu().contiguous().detach().numpy().astype(\"f\").tofile(fname)\n",
        "\n",
        "save_raw(\"conv1_nchw.w\", net.conv1.weight)\n",
        "save_raw(\"conv1_nchw.b\", net.conv1.bias)\n",
        "save_raw(\"fc_nchw.w\",  net.fc.weight)\n",
        "save_raw(\"fc_nchw.b\",  net.fc.bias)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dTGdOyNf_Ug",
        "outputId": "30c4478d-90c0-4dcf-e8e4-43e1d65d9b38"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 469/469 [00:06<00:00, 71.53it/s]\n",
            "Epoch 2/5: 100%|██████████| 469/469 [00:10<00:00, 45.40it/s]\n",
            "Epoch 3/5: 100%|██████████| 469/469 [00:07<00:00, 65.42it/s]\n",
            "Epoch 4/5: 100%|██████████| 469/469 [00:06<00:00, 72.65it/s]\n",
            "Epoch 5/5: 100%|██████████| 469/469 [00:07<00:00, 64.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training done.\n",
            "Test accuracy: 96.95%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CUDA scripts for **GPU**"
      ],
      "metadata": {
        "id": "JhNqGniDhAdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NHWC"
      ],
      "metadata": {
        "id": "Z4P7kE-BhK3C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLg2yk2qX5jY",
        "outputId": "6d5f5e20-dbd0-4d9e-bba5-dca8bd511b50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "gpu_mnist.cu(27): warning #1650-D: result of call is not used\n",
            "      fread(&magic, 4, 1, file);\n",
            "      ^\n",
            "\n",
            "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "gpu_mnist.cu(31): warning #1650-D: result of call is not used\n",
            "      fread(&n, 4, 1, file);\n",
            "      ^\n",
            "\n",
            "gpu_mnist.cu(34): warning #1650-D: result of call is not used\n",
            "          fread(&h, 4, 1, file);\n",
            "          ^\n",
            "\n",
            "gpu_mnist.cu(36): warning #1650-D: result of call is not used\n",
            "          fread(&w, 4, 1, file);\n",
            "          ^\n",
            "\n",
            "gpu_mnist.cu(27): warning #1650-D: result of call is not used\n",
            "      fread(&magic, 4, 1, file);\n",
            "      ^\n",
            "\n",
            "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "gpu_mnist.cu(31): warning #1650-D: result of call is not used\n",
            "      fread(&n, 4, 1, file);\n",
            "      ^\n",
            "\n",
            "gpu_mnist.cu(34): warning #1650-D: result of call is not used\n",
            "          fread(&h, 4, 1, file);\n",
            "          ^\n",
            "\n",
            "gpu_mnist.cu(36): warning #1650-D: result of call is not used\n",
            "          fread(&w, 4, 1, file);\n",
            "          ^\n",
            "\n",
            "gpu_mnist.cu: In function ‘std::vector<unsigned char> load_idx(const string&, int&, int&, int&)’:\n",
            "gpu_mnist.cu:27:6: warning: ignoring return value of ‘size_t fread(void*, size_t, size_t, FILE*)’ declared with attribute ‘warn_unused_result’ [-Wunused-result]\n",
            "   27 |     fread(&magic, 4, 1, file);\n",
            "      |     ~^~~~~~~~~~~~~~~~~~~~\n",
            "gpu_mnist.cu:31:6: warning: ignoring return value of ‘size_t fread(void*, size_t, size_t, FILE*)’ declared with attribute ‘warn_unused_result’ [-Wunused-result]\n",
            "   31 |     fread(&n, 4, 1, file);\n",
            "      |     ~^~~~~~~~~~~~~~~~\n",
            "gpu_mnist.cu:34:6: warning: ignoring return value of ‘size_t fread(void*, size_t, size_t, FILE*)’ declared with attribute ‘warn_unused_result’ [-Wunused-result]\n",
            "   34 |         fread(&h, 4, 1, file);\n",
            "      |      ^  ~~~~~~~~~~~~~\n",
            "gpu_mnist.cu:36:6: warning: ignoring return value of ‘size_t fread(void*, size_t, size_t, FILE*)’ declared with attribute ‘warn_unused_result’ [-Wunused-result]\n",
            "   36 |         fread(&w, 4, 1, file);\n",
            "      |      ^  ~~~~~~~~~~~~~\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "cat > gpu_mnist.cu <<'EOF'\n",
        "#include <cstdint>\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <vector>\n",
        "#include <string>\n",
        "#include <iostream>\n",
        "\n",
        "#ifdef __CUDACC__\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define CUDA_ASSERT(cmd) \\\n",
        "    do { \\\n",
        "        cudaError_t _err = (cmd); \\\n",
        "        if (_err != cudaSuccess) { \\\n",
        "            exit(EXIT_FAILURE); \\\n",
        "        } \\\n",
        "    } while (0)\n",
        "\n",
        "static uint32_t reverse32(uint32_t val) {\n",
        "    return (val>>24) | ((val>>8)&0xFF00) | ((val<<8)&0xFF0000) | (val<<24);\n",
        "}\n",
        "\n",
        "static std::vector<uint8_t> load_idx(const std::string& fname, int& count, int& rows, int& cols) {\n",
        "    FILE* file = fopen(fname.c_str(), \"rb\");\n",
        "    if (!file) { exit(EXIT_FAILURE); }\n",
        "    uint32_t magic, n, h=1, w=1;\n",
        "    fread(&magic, 4, 1, file);\n",
        "    printf(\"Magic: 0x%08X\\n\", magic);\n",
        "    magic = reverse32(magic);\n",
        "    printf(\"Magic : 0x%08X\\n\", magic);\n",
        "    fread(&n, 4, 1, file);\n",
        "    count = n = reverse32(n);\n",
        "    if (magic == 0x00000803) {\n",
        "        fread(&h, 4, 1, file);\n",
        "        rows = h = reverse32(h);\n",
        "        fread(&w, 4, 1, file);\n",
        "        cols = w = reverse32(w);\n",
        "    } else if (magic == 0x00000801) {\n",
        "        rows = cols = 1;\n",
        "    } else {\n",
        "        fclose(file); exit(EXIT_FAILURE);\n",
        "    }\n",
        "    size_t sz = (size_t)count * rows * cols;\n",
        "    std::vector<uint8_t> buf(sz);\n",
        "    size_t got = fread(buf.data(), 1, sz, file);\n",
        "    fclose(file);\n",
        "    return buf;\n",
        "}\n",
        "\n",
        "__global__ void act_relu(float* arr, int len) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < len) {\n",
        "        arr[idx] = fmaxf(0.f, arr[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void pool2x2(const float* __restrict__ src, float* dst, int B, int H, int W, int D) {\n",
        "    int outH = H / 2;\n",
        "    int outW = W / 2;\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int total = B * outH * outW * D;\n",
        "    if (tid >= total) return;\n",
        "    int d  = tid % D;\n",
        "    int x2 = (tid / D) % outW;\n",
        "    int y2 = (tid / D / outW) % outH;\n",
        "    int b  = tid / (D * outH * outW);\n",
        "    int y0 = y2 * 2, x0 = x2 * 2;\n",
        "    float mx = src[((b*H + y0)*W + x0)*D + d];\n",
        "    mx = fmaxf(mx, src[((b*H + y0+1)*W + x0)*D + d]);\n",
        "    mx = fmaxf(mx, src[((b*H + y0)*W + x0+1)*D + d]);\n",
        "    mx = fmaxf(mx, src[((b*H + y0+1)*W + x0+1)*D + d]);\n",
        "    dst[tid] = mx;\n",
        "}\n",
        "\n",
        "__global__ void conv3x3(const float* __restrict__ inp,\n",
        "                        const float* __restrict__ filt,\n",
        "                        const float* __restrict__ bias,\n",
        "                        float*       __restrict__ out,\n",
        "                        int B, int H, int W, int Din, int Dout) {\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int total = B * H * W * Dout;\n",
        "    if (tid >= total) return;\n",
        "    int od = tid % Dout;\n",
        "    int ox = (tid / Dout) % W;\n",
        "    int oy = (tid / Dout / W) % H;\n",
        "    int ob = tid / (Dout * H * W);\n",
        "    float acc = bias[od];\n",
        "    for (int id = 0; id < Din; ++id) {\n",
        "        #pragma unroll\n",
        "        for (int dy = -1; dy <= 1; ++dy) {\n",
        "            for (int dx = -1; dx <= 1; ++dx) {\n",
        "                int y = oy + dy;\n",
        "                int x = ox + dx;\n",
        "                if (y >= 0 && y < H && x >= 0 && x < W) {\n",
        "                    int inp_idx = ((ob*H + y)*W + x)*Din + id;\n",
        "                    int filt_idx = ((od*3 + (dy+1))*3 + (dx+1))*Din + id;\n",
        "                    acc += inp[inp_idx] * filt[filt_idx];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    out[tid] = acc;\n",
        "}\n",
        "\n",
        "__global__ void dense_layer(const float* __restrict__ inp,\n",
        "                           const float* __restrict__ wt,\n",
        "                           const float* __restrict__ bias,\n",
        "                           float*       __restrict__ out,\n",
        "                           int B, int Din, int Dout) {\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int total = B * Dout;\n",
        "    if (tid >= total) return;\n",
        "    int b = tid / Dout;\n",
        "    int o = tid % Dout;\n",
        "    float acc = bias[o];\n",
        "    for (int i = 0; i < Din; ++i) {\n",
        "        acc += inp[b*Din + i] * wt[o*Din + i];\n",
        "    }\n",
        "    out[tid] = acc;\n",
        "}\n",
        "\n",
        "struct DeviceNet {\n",
        "    static constexpr int IN_H=28, IN_W=28, IN_C=1;\n",
        "    static constexpr int CONV_C=8;\n",
        "    static constexpr int CONV_H=28, CONV_W=28, POOL_H=14, POOL_W=14;\n",
        "    static constexpr int FCIN = POOL_H*POOL_W*CONV_C;\n",
        "    static constexpr int FCOUT=10;\n",
        "\n",
        "    int batch;\n",
        "    float *g_img, *g_conv_w, *g_conv_b, *g_conv_out, *g_pool,\n",
        "          *g_fc_w, *g_fc_b, *g_fc_out;\n",
        "\n",
        "    DeviceNet(int n): batch(n) {\n",
        "        auto alloc=[&](float** ptr, size_t sz, const char* label){\n",
        "            CUDA_ASSERT(cudaMalloc(ptr, sz*sizeof(float)));\n",
        "        };\n",
        "        alloc(&g_img,     (size_t)batch*IN_H*IN_W*IN_C, \"g_img\");\n",
        "        alloc(&g_conv_w,  (size_t)CONV_C*3*3*IN_C,  \"g_conv_w\"); alloc(&g_conv_b, CONV_C, \"g_conv_b\");\n",
        "        alloc(&g_conv_out,(size_t)batch*CONV_H*CONV_W*CONV_C, \"g_conv_out\");  alloc(&g_pool, (size_t)batch*POOL_H*POOL_W*CONV_C, \"g_pool\");\n",
        "        alloc(&g_fc_w,    (size_t)FCOUT*FCIN, \"g_fc_w\"); alloc(&g_fc_b, FCOUT, \"g_fc_b\");\n",
        "        alloc(&g_fc_out,  (size_t)batch*FCOUT,   \"g_fc_out\");\n",
        "        auto loadw = [&](const char* fname, float* dptr, size_t len){\n",
        "            std::vector<float> buf(len);\n",
        "            FILE* file = fopen(fname, \"rb\");\n",
        "            size_t got = fread(buf.data(), sizeof(float), len, file);\n",
        "            fclose(file);\n",
        "            CUDA_ASSERT(cudaMemcpy(dptr, buf.data(), len*sizeof(float), cudaMemcpyHostToDevice));\n",
        "        };\n",
        "        loadw(\"conv1.w\", g_conv_w, (size_t)CONV_C*3*3*IN_C);\n",
        "        loadw(\"conv1.b\", g_conv_b, (size_t)CONV_C);\n",
        "        loadw(\"fc.w\",    g_fc_w, (size_t)FCOUT*FCIN);\n",
        "        loadw(\"fc.b\",    g_fc_b, (size_t)FCOUT);\n",
        "        printf(\"Model params\\n\");\n",
        "    }\n",
        "    ~DeviceNet() {\n",
        "        cudaFree(g_img);\n",
        "        cudaFree(g_conv_w);\n",
        "        cudaFree(g_conv_b);\n",
        "        cudaFree(g_conv_out);\n",
        "        cudaFree(g_pool);\n",
        "        cudaFree(g_fc_w);\n",
        "        cudaFree(g_fc_b);\n",
        "        cudaFree(g_fc_out);\n",
        "    }\n",
        "    void propagate(const uint8_t* img) {\n",
        "        std::vector<float> h_img((size_t)batch*IN_H*IN_W*IN_C);\n",
        "        for (size_t i = 0; i < h_img.size(); ++i) h_img[i] = img[i] / 255.f;\n",
        "        CUDA_ASSERT(cudaMemcpy(g_img, h_img.data(), h_img.size()*sizeof(float), cudaMemcpyHostToDevice));\n",
        "        int blk = 256;\n",
        "        auto grid = [&](int n) { return dim3((n + blk - 1) / blk); };\n",
        "        int conv_sz = batch*CONV_H*CONV_W*CONV_C;\n",
        "        conv3x3<<<grid(conv_sz), blk>>>(g_img, g_conv_w, g_conv_b, g_conv_out, batch, IN_H, IN_W, IN_C, CONV_C);\n",
        "        act_relu<<<grid(conv_sz), blk>>>(g_conv_out, conv_sz);\n",
        "        int pool_sz = batch*POOL_H*POOL_W*CONV_C;\n",
        "        pool2x2<<<grid(pool_sz), blk>>>(g_conv_out, g_pool, batch, CONV_H, CONV_W, CONV_C);\n",
        "        int fc_sz = batch*FCOUT;\n",
        "        dense_layer<<<grid(fc_sz), blk>>>(g_pool, g_fc_w, g_fc_b, g_fc_out, batch, FCIN, FCOUT);\n",
        "        CUDA_ASSERT(cudaGetLastError());\n",
        "    }\n",
        "    double timeit(const uint8_t* img) {\n",
        "        cudaEvent_t st,en; CUDA_ASSERT(cudaEventCreate(&st)); CUDA_ASSERT(cudaEventCreate(&en));\n",
        "        CUDA_ASSERT(cudaEventRecord(st));\n",
        "        propagate(img);\n",
        "        CUDA_ASSERT(cudaEventRecord(en));\n",
        "        CUDA_ASSERT(cudaEventSynchronize(en));\n",
        "        float ms; CUDA_ASSERT(cudaEventElapsedTime(&ms, st, en));\n",
        "        CUDA_ASSERT(cudaEventDestroy(st)); CUDA_ASSERT(cudaEventDestroy(en));\n",
        "        return ms/1000.0;\n",
        "    }\n",
        "    int assess(const uint8_t* img, const uint8_t* lbl) {\n",
        "        printf(\"Running forward for accuracy...\\n\");\n",
        "        propagate(img);\n",
        "        std::vector<float> h_out((size_t)batch*FCOUT);\n",
        "        CUDA_ASSERT(cudaMemcpy(h_out.data(), g_fc_out, h_out.size()*sizeof(float), cudaMemcpyDeviceToHost));\n",
        "        int nright = 0;\n",
        "        for (int i = 0; i < batch; ++i) {\n",
        "            int pred = 0; float maxv = -1e9f;\n",
        "            for (int j = 0; j < FCOUT; ++j) {\n",
        "                float v = h_out[i*FCOUT + j];\n",
        "                if (v > maxv) { maxv = v; pred = j; }\n",
        "            }\n",
        "            if (pred == lbl[i]) ++nright;\n",
        "        }\n",
        "        printf(\"Accuracy done.\\n\");\n",
        "        return nright;\n",
        "    }\n",
        "};\n",
        "\n",
        "static void launch_gpu() {\n",
        "    int nImg, rImg, cImg, nLbl, rLbl, cLbl;\n",
        "    auto imgs = load_idx(\"t10k-images-idx3-ubyte\", nImg, rImg, cImg);\n",
        "    auto lbls = load_idx(\"t10k-labels-idx1-ubyte\", nLbl, rLbl, cLbl);\n",
        "    if (nImg != 10000 || nLbl != 10000 || nImg != nLbl) {\n",
        "        return;\n",
        "    }\n",
        "    DeviceNet model(nImg);\n",
        "    double sec = model.timeit(imgs.data());\n",
        "    int top1 = model.assess(imgs.data(), lbls.data());\n",
        "    double throughput = nImg / sec;\n",
        "    double acc_percent = top1 * 100.0 / nImg;\n",
        "    std::cout << \"========================================\\n\";\n",
        "    std::cout << \"  Throughput: \" << throughput << \" img/s\\n\";\n",
        "    std::cout << \"  Accuracy:   \" << acc_percent << \"% (\" << top1 << \"/\" << nImg << \")\\n\";\n",
        "    std::cout << \"========================================\\n\";\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    launch_gpu();\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "#else\n",
        "#error \"error\"\n",
        "#endif\n",
        "\n",
        "EOF\n",
        "\n",
        "nvcc gpu_mnist.cu -O3 -o gpu_mnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 gpu_mnist.cu -O3 -o gpu_mnist\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-HzjnjjZvT2",
        "outputId": "c89d4760-3b6b-4b37-d66a-50ab944652ea"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mgpu_mnist.cu(27)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1650-D: result of call is not used\n",
            "      fread(&magic, 4, 1, file);\n",
            "      ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mgpu_mnist.cu(31)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1650-D: result of call is not used\n",
            "      fread(&n, 4, 1, file);\n",
            "      ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mgpu_mnist.cu(34)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1650-D: result of call is not used\n",
            "          fread(&h, 4, 1, file);\n",
            "          ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mgpu_mnist.cu(36)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1650-D: result of call is not used\n",
            "          fread(&w, 4, 1, file);\n",
            "          ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mgpu_mnist.cu(27)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1650-D: result of call is not used\n",
            "      fread(&magic, 4, 1, file);\n",
            "      ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mgpu_mnist.cu(31)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1650-D: result of call is not used\n",
            "      fread(&n, 4, 1, file);\n",
            "      ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mgpu_mnist.cu(34)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1650-D: result of call is not used\n",
            "          fread(&h, 4, 1, file);\n",
            "          ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mgpu_mnist.cu(36)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1650-D: result of call is not used\n",
            "          fread(&w, 4, 1, file);\n",
            "          ^\n",
            "\n",
            "\u001b[01m\u001b[Kgpu_mnist.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::vector<unsigned char> load_idx(const string&, int&, int&, int&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kgpu_mnist.cu:27:6:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   27 | \u001b[01;35m\u001b[K    fread(&magic, 4, 1, f\u001b[m\u001b[Kile);\n",
            "      |     \u001b[01;35m\u001b[K~^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgpu_mnist.cu:31:6:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   31 | \u001b[01;35m\u001b[K    fread(&n, 4, 1, f\u001b[m\u001b[Kile);\n",
            "      |     \u001b[01;35m\u001b[K~^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgpu_mnist.cu:34:6:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   34 | \u001b[01;35m\u001b[K        fread(&h, 4, \u001b[m\u001b[K1, file);\n",
            "      |      \u001b[01;35m\u001b[K^\u001b[m\u001b[K  \u001b[01;35m\u001b[K~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgpu_mnist.cu:36:6:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   36 | \u001b[01;35m\u001b[K        fread(&w, 4, \u001b[m\u001b[K1, file);\n",
            "      |      \u001b[01;35m\u001b[K^\u001b[m\u001b[K  \u001b[01;35m\u001b[K~~~~~~~~~~~~~\u001b[m\u001b[K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./gpu_mnist\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBfV2QV3Yi_9",
        "outputId": "a5ede799-0fae-4b1f-fb16-cb4d78e5ac80"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Magic: 0x03080000\n",
            "Magic : 0x00000803\n",
            "Magic: 0x01080000\n",
            "Magic : 0x00000801\n",
            "Model params\n",
            "Running forward for accuracy...\n",
            "Accuracy done.\n",
            "========================================\n",
            "  Throughput: 214563 img/s\n",
            "  Accuracy:   96.92% (9692/10000)\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NCHW"
      ],
      "metadata": {
        "id": "xbS-IhSOhOPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > gpu_mnist_nchw.cu <<'EOF'\n",
        "#include <cstdint>\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <vector>\n",
        "#include <string>\n",
        "#include <iostream>\n",
        "#include <chrono>\n",
        "\n",
        "#ifdef __CUDACC__\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "\n",
        "#define CUDA_CHECK(call) \\\n",
        "    do { \\\n",
        "        cudaError_t _e = (call); \\\n",
        "        if (_e != cudaSuccess) { \\\n",
        "            exit(EXIT_FAILURE); \\\n",
        "        } \\\n",
        "    } while (0)\n",
        "\n",
        "static uint32_t swap32(uint32_t v) {\n",
        "    return (v>>24) | ((v>>8)&0xFF00) | ((v<<8)&0xFF0000) | (v<<24);\n",
        "}\n",
        "\n",
        "static std::vector<uint8_t> read_idx(const std::string& path, int& n, int& r, int& c) {\n",
        "    FILE* f = fopen(path.c_str(), \"rb\");\n",
        "    if (!f) { exit(EXIT_FAILURE); }\n",
        "    uint32_t magic, num_items, rows=1, cols=1;\n",
        "    fread(&magic, 4, 1, f);\n",
        "    magic = swap32(magic);\n",
        "    fread(&num_items, 4, 1, f);\n",
        "    n = num_items = swap32(num_items);\n",
        "\n",
        "    if (magic == 0x00000803) {\n",
        "        fread(&rows, 4, 1, f);\n",
        "        r = rows = swap32(rows);\n",
        "        fread(&cols, 4, 1, f);\n",
        "        c = cols = swap32(cols);\n",
        "    } else if (magic == 0x00000801) {\n",
        "        r = c = 1;\n",
        "    } else {\n",
        "        fclose(f); exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    size_t data_size = (size_t)n * r * c;\n",
        "    std::vector<uint8_t> data(data_size);\n",
        "    size_t bytes_read = fread(data.data(), 1, data_size, f);\n",
        "    if (bytes_read != data_size) { fclose(f); exit(EXIT_FAILURE); }\n",
        "    fclose(f);\n",
        "    return data;\n",
        "}\n",
        "\n",
        "__global__ void relu_kernel(float* x, int n) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < n) {\n",
        "        x[i] = fmaxf(0.f, x[i]);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void maxpool2x2_kernel(const float* __restrict__ x, float* y, int N, int C, int H, int W) {\n",
        "    int H_out = H / 2;\n",
        "    int W_out = W / 2;\n",
        "\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int tot = N * C * H_out * W_out;\n",
        "    if (idx >= tot) return;\n",
        "\n",
        "    int w2 = idx % W_out;\n",
        "    int h2 = (idx / W_out) % H_out;\n",
        "    int c  = (idx / (W_out * H_out)) % C;\n",
        "    int n  = idx / (C * H_out * W_out);\n",
        "\n",
        "    int h0 = h2 * 2, w0 = w2 * 2;\n",
        "\n",
        "    const float* x_offset = x + (n * C + c) * H * W;\n",
        "    float m = x_offset[h0 * W + w0];\n",
        "    m = fmaxf(m, x_offset[(h0+1) * W + w0]);\n",
        "    m = fmaxf(m, x_offset[h0 * W + w0+1]);\n",
        "    m = fmaxf(m, x_offset[(h0+1) * W + w0+1]);\n",
        "    y[idx] = m;\n",
        "}\n",
        "\n",
        "__global__ void conv3x3_kernel(const float* __restrict__ x,\n",
        "                               const float* __restrict__ w,\n",
        "                               const float* __restrict__ b,\n",
        "                               float*       __restrict__ y,\n",
        "                               int N, int H, int W, int Cin, int Cout) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int tot = N * Cout * H * W;\n",
        "    if (idx >= tot) return;\n",
        "\n",
        "    int w0 = idx % W;\n",
        "    int h0 = (idx / W) % H;\n",
        "    int co = (idx / (H * W)) % Cout;\n",
        "    int n  = idx / (Cout * H * W);\n",
        "\n",
        "    float acc = b[co];\n",
        "\n",
        "    for (int ci = 0; ci < Cin; ++ci) {\n",
        "        const float* w_offset = w + (co * Cin + ci) * 3 * 3;\n",
        "        const float* x_offset = x + (n * Cin + ci) * H * W;\n",
        "\n",
        "        for (int dr = -1; dr <= 1; ++dr) {\n",
        "            for (int dc = -1; dc <= 1; ++dc) {\n",
        "                int h = h0 + dr;\n",
        "                int c = w0 + dc;\n",
        "\n",
        "                if (h >= 0 && h < H && c >= 0 && c < W) {\n",
        "                    int x_idx = h * W + c;\n",
        "                    int w_idx = (dr+1) * 3 + (dc+1);\n",
        "                    acc += x_offset[x_idx] * w_offset[w_idx];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    y[idx] = acc;\n",
        "}\n",
        "\n",
        "__global__ void fc_kernel(const float* __restrict__ x,\n",
        "                          const float* __restrict__ w,\n",
        "                          const float* __restrict__ b,\n",
        "                          float*       __restrict__ y,\n",
        "                          int N, int D, int K) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int tot = N * K;\n",
        "    if (idx >= tot) return;\n",
        "\n",
        "    int n = idx / K;\n",
        "    int k = idx % K;\n",
        "\n",
        "    float acc = b[k];\n",
        "\n",
        "    for (int d = 0; d < D; ++d) {\n",
        "        acc += x[n*D + d] * w[k*D + d];\n",
        "    }\n",
        "    y[idx] = acc;\n",
        "}\n",
        "\n",
        "struct GpuNet {\n",
        "    static constexpr int H0=28, W0=28, C0=1;\n",
        "    static constexpr int C1=8;\n",
        "    static constexpr int H1=28, W1=28, H1p=14, W1p=14;\n",
        "    static constexpr int FC_IN = H1p*W1p*C1;\n",
        "    static constexpr int FC_OUT=10;\n",
        "\n",
        "    int N;\n",
        "\n",
        "    float *d_x, *d_c1w, *d_c1b, *d_y1, *d_y1p,\n",
        "          *d_fc_w, *d_fc_b, *d_fc_out;\n",
        "\n",
        "    GpuNet(int nImg): N(nImg) {\n",
        "\n",
        "        auto mal=[&](float** p, size_t s, const char* name){\n",
        "            CUDA_CHECK(cudaMalloc(p, s*sizeof(float)));\n",
        "        };\n",
        "\n",
        "        mal(&d_x,     (size_t)N*C0*H0*W0, \"d_x\");\n",
        "        mal(&d_c1w,   (size_t)C1*3*3*C0,  \"d_c1w\"); mal(&d_c1b, C1, \"d_c1b\");\n",
        "        mal(&d_y1,    (size_t)N*C1*H1*W1, \"d_y1\");  mal(&d_y1p, (size_t)N*C1*H1p*W1p, \"d_y1p\");\n",
        "        mal(&d_fc_w,  (size_t)FC_OUT*FC_IN, \"d_fc_w\"); mal(&d_fc_b, FC_OUT, \"d_fc_b\");\n",
        "        mal(&d_fc_out,(size_t)N*FC_OUT,   \"d_fc_out\");\n",
        "\n",
        "        auto load_weights = [&](const char* prefix, char suffix, float* d_ptr, size_t num_elements) {\n",
        "            char fpath[256];\n",
        "            snprintf(fpath, sizeof(fpath), \"%s_nchw.%c\", prefix, suffix);\n",
        "            FILE *f = fopen(fpath, \"rb\");\n",
        "            if (!f) { exit(EXIT_FAILURE); }\n",
        "            std::vector<float> h_buf(num_elements);\n",
        "            size_t read_count = fread(h_buf.data(), sizeof(float), num_elements, f);\n",
        "            fclose(f);\n",
        "            if (read_count != num_elements) { exit(EXIT_FAILURE); }\n",
        "            CUDA_CHECK(cudaMemcpy(d_ptr, h_buf.data(), num_elements * sizeof(float), cudaMemcpyHostToDevice));\n",
        "        };\n",
        "\n",
        "        load_weights(\"conv1\", 'w', d_c1w, C1*C0*3*3);\n",
        "        load_weights(\"conv1\", 'b', d_c1b, C1);\n",
        "        load_weights(\"fc\",    'w', d_fc_w, FC_OUT*FC_IN);\n",
        "        load_weights(\"fc\",    'b', d_fc_b, FC_OUT);\n",
        "    }\n",
        "\n",
        "    ~GpuNet() {\n",
        "        cudaFree(d_x); cudaFree(d_c1w); cudaFree(d_c1b); cudaFree(d_y1);\n",
        "        cudaFree(d_y1p); cudaFree(d_fc_w); cudaFree(d_fc_b); cudaFree(d_fc_out);\n",
        "    }\n",
        "\n",
        "    void forward(const uint8_t* img) {\n",
        "        std::vector<float> h_x(N * C0 * H0 * W0);\n",
        "        for (size_t i = 0; i < h_x.size(); ++i) {\n",
        "            h_x[i] = img[i] / 255.0f;\n",
        "        }\n",
        "        CUDA_CHECK(cudaMemcpy(d_x, h_x.data(), h_x.size() * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "        int threads = 256;\n",
        "        auto grid = [&](int total_elems) { return (total_elems + threads - 1) / threads; };\n",
        "\n",
        "        conv3x3_kernel<<<grid(N*C1*H1*W1), threads>>>(\n",
        "            d_x, d_c1w, d_c1b, d_y1, N, H1, W1, C0, C1);\n",
        "        relu_kernel<<<grid(N*C1*H1*W1), threads>>>(d_y1, N*C1*H1*W1);\n",
        "        maxpool2x2_kernel<<<grid(N*C1*H1p*W1p), threads>>>(\n",
        "            d_y1, d_y1p, N, C1, H1, W1);\n",
        "\n",
        "        fc_kernel<<<grid(N*FC_OUT), threads>>>(\n",
        "            d_y1p, d_fc_w, d_fc_b, d_fc_out, N, FC_IN, FC_OUT);\n",
        "    }\n",
        "\n",
        "    int accuracy(const uint8_t* img, const uint8_t* lbl) {\n",
        "        forward(img);\n",
        "\n",
        "        std::vector<float> h_fc_out(N * FC_OUT);\n",
        "        CUDA_CHECK(cudaMemcpy(h_fc_out.data(), d_fc_out, h_fc_out.size() * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "        int correct = 0;\n",
        "        for (int i = 0; i < N; ++i) {\n",
        "            float max_val = -1e9;\n",
        "            int pred = -1;\n",
        "            for (int j = 0; j < FC_OUT; ++j) {\n",
        "                if (h_fc_out[i * FC_OUT + j] > max_val) {\n",
        "                    max_val = h_fc_out[i * FC_OUT + j];\n",
        "                    pred = j;\n",
        "                }\n",
        "            }\n",
        "            if (pred == lbl[i]) {\n",
        "                correct++;\n",
        "            }\n",
        "        }\n",
        "        return correct;\n",
        "    }\n",
        "};\n",
        "\n",
        "static void run_gpu() {\n",
        "    int n_test, h_test, w_test, n_lbl, r_lbl, c_lbl;\n",
        "    std::vector<uint8_t> test_img = read_idx(\"t10k-images-idx3-ubyte\", n_test, h_test, w_test);\n",
        "    std::vector<uint8_t> test_lbl = read_idx(\"t10k-labels-idx1-ubyte\", n_lbl, r_lbl, c_lbl);\n",
        "    if (n_test != n_lbl) { fprintf(stderr, \"[ERROR] Mismatch between image count (%d) and label count (%d)\\n\", n_test, n_lbl); exit(EXIT_FAILURE); }\n",
        "\n",
        "    GpuNet net(n_test);\n",
        "\n",
        "    double sec = 0.0;\n",
        "    int num_runs = 10;\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "    for (int i = 0; i < num_runs; ++i) {\n",
        "        net.forward(test_img.data());\n",
        "    }\n",
        "    CUDA_CHECK(cudaDeviceSynchronize());\n",
        "    auto end = std::chrono::high_resolution_clock::now();\n",
        "    std::chrono::duration<double> duration = end - start;\n",
        "    sec = duration.count() / num_runs;\n",
        "\n",
        "    int top1 = net.accuracy(test_img.data(), test_lbl.data());\n",
        "\n",
        "    double throughput = n_test / sec;\n",
        "    double acc_percent = top1 * 100.0 / n_test;\n",
        "    std::cout << \"----------------------------------------\\n\";\n",
        "    std::cout << \"  Throughput: \" << throughput << \" img/s\\n\";\n",
        "    std::cout << \"  Accuracy:   \" << acc_percent << \"% (\" << top1 << \"/\" << n_test << \")\\n\";\n",
        "    std::cout << \"----------------------------------------\\n\";\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    run_gpu();\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "#else // __CUDACC__ not defined\n",
        "int main() {\n",
        "    return 1;\n",
        "}\n",
        "#endif // __CUDACC__\n",
        "EOF\n",
        "\n",
        "nvcc -std=c++14 -arch=sm_75 gpu_mnist_nchw.cu -O3 -o gpu_mnist_nchw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7cUROSlbggr",
        "outputId": "1f10bf32-2ccc-492d-d1e2-193a7d8bcc08"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "gpu_mnist_nchw.cu(29): warning #1650-D: result of call is not used\n",
            "      fread(&magic, 4, 1, f);\n",
            "      ^\n",
            "\n",
            "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "gpu_mnist_nchw.cu(31): warning #1650-D: result of call is not used\n",
            "      fread(&num_items, 4, 1, f);\n",
            "      ^\n",
            "\n",
            "gpu_mnist_nchw.cu(35): warning #1650-D: result of call is not used\n",
            "          fread(&rows, 4, 1, f);\n",
            "          ^\n",
            "\n",
            "gpu_mnist_nchw.cu(37): warning #1650-D: result of call is not used\n",
            "          fread(&cols, 4, 1, f);\n",
            "          ^\n",
            "\n",
            "gpu_mnist_nchw.cu(29): warning #1650-D: result of call is not used\n",
            "      fread(&magic, 4, 1, f);\n",
            "      ^\n",
            "\n",
            "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "gpu_mnist_nchw.cu(31): warning #1650-D: result of call is not used\n",
            "      fread(&num_items, 4, 1, f);\n",
            "      ^\n",
            "\n",
            "gpu_mnist_nchw.cu(35): warning #1650-D: result of call is not used\n",
            "          fread(&rows, 4, 1, f);\n",
            "          ^\n",
            "\n",
            "gpu_mnist_nchw.cu(37): warning #1650-D: result of call is not used\n",
            "          fread(&cols, 4, 1, f);\n",
            "          ^\n",
            "\n",
            "gpu_mnist_nchw.cu: In function ‘std::vector<unsigned char> read_idx(const string&, int&, int&, int&)’:\n",
            "gpu_mnist_nchw.cu:29:6: warning: ignoring return value of ‘size_t fread(void*, size_t, size_t, FILE*)’ declared with attribute ‘warn_unused_result’ [-Wunused-result]\n",
            "   29 |     fread(&magic, 4, 1, f);\n",
            "      |     ~^~~~~~~~~~~~~~~~~\n",
            "gpu_mnist_nchw.cu:31:6: warning: ignoring return value of ‘size_t fread(void*, size_t, size_t, FILE*)’ declared with attribute ‘warn_unused_result’ [-Wunused-result]\n",
            "   31 |     fread(&num_items, 4, 1, f);\n",
            "      |     ~^~~~~~~~~~~~~~~~~~~~~\n",
            "gpu_mnist_nchw.cu:35:6: warning: ignoring return value of ‘size_t fread(void*, size_t, size_t, FILE*)’ declared with attribute ‘warn_unused_result’ [-Wunused-result]\n",
            "   35 |         fread(&rows, 4, 1, f);\n",
            "      |      ^  ~~~~~~~~~~~~~\n",
            "gpu_mnist_nchw.cu:37:6: warning: ignoring return value of ‘size_t fread(void*, size_t, size_t, FILE*)’ declared with attribute ‘warn_unused_result’ [-Wunused-result]\n",
            "   37 |         fread(&cols, 4, 1, f);\n",
            "      |      ^  ~~~~~~~~~~~~~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 gpu_mnist_nchw.cu -O3 -o gpu_mnist_nchw\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeFewxZobpio",
        "outputId": "143826c8-3c92-47cd-c1bd-2723758c91ae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mgpu_mnist_nchw.cu(29)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1650-D: result of call is not used\n",
            "      fread(&magic, 4, 1, f);\n",
            "      ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mgpu_mnist_nchw.cu(31)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1650-D: result of call is not used\n",
            "      fread(&num_items, 4, 1, f);\n",
            "      ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mgpu_mnist_nchw.cu(35)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1650-D: result of call is not used\n",
            "          fread(&rows, 4, 1, f);\n",
            "          ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mgpu_mnist_nchw.cu(37)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1650-D: result of call is not used\n",
            "          fread(&cols, 4, 1, f);\n",
            "          ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mgpu_mnist_nchw.cu(29)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1650-D: result of call is not used\n",
            "      fread(&magic, 4, 1, f);\n",
            "      ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mgpu_mnist_nchw.cu(31)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1650-D: result of call is not used\n",
            "      fread(&num_items, 4, 1, f);\n",
            "      ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mgpu_mnist_nchw.cu(35)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1650-D: result of call is not used\n",
            "          fread(&rows, 4, 1, f);\n",
            "          ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mgpu_mnist_nchw.cu(37)\u001b[0m: \u001b[01;35mwarning\u001b[0m #1650-D: result of call is not used\n",
            "          fread(&cols, 4, 1, f);\n",
            "          ^\n",
            "\n",
            "\u001b[01m\u001b[Kgpu_mnist_nchw.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::vector<unsigned char> read_idx(const string&, int&, int&, int&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kgpu_mnist_nchw.cu:29:6:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   29 | \u001b[01;35m\u001b[K    fread(&magic, 4, 1\u001b[m\u001b[K, f);\n",
            "      |     \u001b[01;35m\u001b[K~^~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgpu_mnist_nchw.cu:31:6:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   31 | \u001b[01;35m\u001b[K    fread(&num_items, 4, 1\u001b[m\u001b[K, f);\n",
            "      |     \u001b[01;35m\u001b[K~^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgpu_mnist_nchw.cu:35:6:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   35 | \u001b[01;35m\u001b[K        fread(&rows, \u001b[m\u001b[K4, 1, f);\n",
            "      |      \u001b[01;35m\u001b[K^\u001b[m\u001b[K  \u001b[01;35m\u001b[K~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kgpu_mnist_nchw.cu:37:6:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   37 | \u001b[01;35m\u001b[K        fread(&cols, \u001b[m\u001b[K4, 1, f);\n",
            "      |      \u001b[01;35m\u001b[K^\u001b[m\u001b[K  \u001b[01;35m\u001b[K~~~~~~~~~~~~~\u001b[m\u001b[K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./gpu_mnist_nchw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LHZWnDGcBKy",
        "outputId": "6d9e5e35-d904-461f-8699-11ab54f3bce4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "  Throughput: 398298 img/s\n",
            "  Accuracy:   96.95% (9695/10000)\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CPU Scripts"
      ],
      "metadata": {
        "id": "APTwZlzFha33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CPU Unoptimized"
      ],
      "metadata": {
        "id": "xmGbuPb0hgJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "\n",
        "class NetArch(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_a = torch.nn.Conv2d(1, 8, 3, padding=1)\n",
        "        self.layer_b = torch.nn.Linear(14*14*8, 10)\n",
        "    def forward(self, z):\n",
        "        z = torch.relu(self.layer_a(z))\n",
        "        z = torch.max_pool2d(z, 2)\n",
        "        z = self.layer_b(z.view(z.size(0), -1))\n",
        "        return z\n",
        "\n",
        "def tensor_from_file(path, dims):\n",
        "    data = np.fromfile(path, dtype=np.float32)\n",
        "    return torch.from_numpy(data.reshape(dims))\n",
        "\n",
        "model = NetArch()\n",
        "weights_a = tensor_from_file('conv1.w', (8, 3, 3, 1)).permute(0, 3, 1, 2).contiguous()\n",
        "biases_a = tensor_from_file('conv1.b', (8,))\n",
        "weights_b = tensor_from_file('fc.w', (10, 1568))\n",
        "biases_b = tensor_from_file('fc.b', (10,))\n",
        "indices = []\n",
        "for i in range(14):\n",
        "    for j in range(14):\n",
        "        for k in range(8):\n",
        "            indices.append(k*14*14 + i*14 + j)\n",
        "reverse = np.argsort(indices)\n",
        "weights_b2 = weights_b[:, reverse]\n",
        "with torch.no_grad():\n",
        "    model.layer_a.weight.copy_(weights_a)\n",
        "    model.layer_a.bias.copy_(biases_a)\n",
        "    model.layer_b.weight.copy_(weights_b2)\n",
        "    model.layer_b.bias.copy_(biases_b)\n",
        "model.eval()\n",
        "\n",
        "transformer = transforms.Compose([transforms.ToTensor()])\n",
        "data_iter = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('.', train=False, download=True, transform=transformer),\n",
        "    batch_size=1000, shuffle=False)\n",
        "\n",
        "hits = count = 0\n",
        "t0 = time.time()\n",
        "with torch.no_grad():\n",
        "    for u, v in data_iter:\n",
        "        y_hat = model(u).argmax(1)\n",
        "        hits += (y_hat == v).sum().item()\n",
        "        count += v.size(0)\n",
        "t1 = time.time()\n",
        "acc = 100*hits/count\n",
        "thr = count/(t1-t0)\n",
        "print(f\"CPU inference accuracy: {acc:.2f}% throughput: {thr:.1f} img/s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_NPEQ76hlUC",
        "outputId": "e65300eb-3303-48a9-9512-2dbbebe8e63e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU inference accuracy: 96.92% throughput: 5108.4 img/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CPU Optimized"
      ],
      "metadata": {
        "id": "T0Mo2Pd8hp3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "import os\n",
        "import multiprocessing\n",
        "\n",
        "os.environ[\"OMP_NUM_THREADS\"] = str(multiprocessing.cpu_count())\n",
        "os.environ[\"MKL_NUM_THREADS\"] = str(multiprocessing.cpu_count())\n",
        "torch.set_num_threads(multiprocessing.cpu_count())\n",
        "\n",
        "def tensorfile(fp, shp):\n",
        "    dat = np.fromfile(fp, dtype=np.float32)\n",
        "    return torch.from_numpy(dat.reshape(shp))\n",
        "\n",
        "class VariantNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mod1 = torch.nn.Conv2d(1, 8, 3, padding=1)\n",
        "        self.mod2 = torch.nn.Linear(14*14*8, 10)\n",
        "    def forward(self, z):\n",
        "        if z.dtype != torch.float32:\n",
        "            z = z.float()\n",
        "        if z.dim() == 3:\n",
        "            z = z.unsqueeze(0)\n",
        "        y = torch.relu(self.mod1(z))\n",
        "        y = torch.max_pool2d(y, 2)\n",
        "        y = self.mod2(y.view(y.size(0), -1))\n",
        "        return y\n",
        "\n",
        "def run_cpu():\n",
        "    cpus = multiprocessing.cpu_count()\n",
        "    mdl = VariantNet()\n",
        "    t_init = time.time()\n",
        "    w_a = tensorfile('conv1.w', (8, 3, 3, 1)).permute(0, 3, 1, 2).contiguous()\n",
        "    b_a = tensorfile('conv1.b', (8,))\n",
        "    w_b = tensorfile('fc.w', (10, 1568))\n",
        "    b_b = tensorfile('fc.b', (10,))\n",
        "    idxs = [(c*14*14 + h*14 + w) for h in range(14) for w in range(14) for c in range(8)]\n",
        "    rev = np.argsort(idxs)\n",
        "    w_b2 = w_b[:, rev]\n",
        "    with torch.no_grad():\n",
        "        mdl.mod1.weight.copy_(w_a)\n",
        "        mdl.mod1.bias.copy_(b_a)\n",
        "        mdl.mod2.weight.copy_(w_b2)\n",
        "        mdl.mod2.bias.copy_(b_b)\n",
        "    scripted_net = torch.jit.script(mdl)\n",
        "    scripted_net.eval()\n",
        "    tfm = transforms.Compose([transforms.ToTensor()])\n",
        "    batch_sizes = list(range(1000, 10001, 1000))\n",
        "    for N in batch_sizes:\n",
        "        loader = torch.utils.data.DataLoader(\n",
        "            datasets.MNIST('.', train=False, download=True, transform=tfm),\n",
        "            batch_size=N, shuffle=False, pin_memory=True,\n",
        "            num_workers=min(4, cpus), persistent_workers=True)\n",
        "        total = match = 0\n",
        "        t1 = time.time()\n",
        "        for u, _ in torch.utils.data.DataLoader(\n",
        "                datasets.MNIST('.', train=False, transform=tfm), batch_size=10):\n",
        "            with torch.no_grad():\n",
        "                scripted_net(u)\n",
        "            break\n",
        "        with torch.no_grad():\n",
        "            for u, v in loader:\n",
        "                pred = scripted_net(u).argmax(1)\n",
        "                match += (pred == v).sum().item()\n",
        "                total += v.size(0)\n",
        "        t2 = time.time()\n",
        "        acc = 100*match/total\n",
        "        thr = total/(t2-t1)\n",
        "        print(f\"CPU inference accuracy: {acc:.2f}% throughput: {thr:.1f} img/s\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_cpu()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6IGAqoYhnfu",
        "outputId": "1c14a682-f9eb-485a-a9c4-176b2f7b0fb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU inference accuracy: 96.92% throughput: 5696.2 img/s\n",
            "CPU inference accuracy: 96.92% throughput: 4951.0 img/s\n",
            "CPU inference accuracy: 96.92% throughput: 3414.0 img/s\n",
            "CPU inference accuracy: 96.92% throughput: 4010.8 img/s\n",
            "CPU inference accuracy: 96.92% throughput: 5105.2 img/s\n"
          ]
        }
      ]
    }
  ]
}